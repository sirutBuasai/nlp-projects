{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 525 Assignment 2\n",
    "Sirut Buasai, sbuasai2@wpi.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "embeddings = gensim_api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Labels Based On Rating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv file\n",
    "raw_data = pd.read_csv('Reviews.csv')\n",
    "\n",
    "# create labels based on score (label 1 when score >= 3, label 0 when score < 3)\n",
    "raw_data['labels'] = np.where(raw_data.Score >= 3, 1, 0)\n",
    "groups = raw_data.groupby('labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    82037\n",
       "1    82037\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample balanced data\n",
    "sampled_data = groups.apply(lambda x: x.sample(groups.size().min()).reset_index(drop=True))\n",
    "sampled_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuations, Tokenize, Remove Stop Words, and Lemmatize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>labels</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stop_removed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>100942</td>\n",
       "      <td>B0016ZU83M</td>\n",
       "      <td>A3E3EQ39FZTZEK</td>\n",
       "      <td>sh sh</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1347667200</td>\n",
       "      <td>Much less flaovr nearly same calories</td>\n",
       "      <td>these are for people that cannot tolerate suga...</td>\n",
       "      <td>0</td>\n",
       "      <td>[these, are, for, people, that, can, not, tole...</td>\n",
       "      <td>[people, tolerate, sugar, low, cal, slightly, ...</td>\n",
       "      <td>[people, tolerate, sugar, low, cal, slightly, ...</td>\n",
       "      <td>people tolerate sugar low cal slightly lower c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>480042</td>\n",
       "      <td>B003IHO8OG</td>\n",
       "      <td>A11JJPVSHVJM0S</td>\n",
       "      <td>Nicole Taylor</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1287705600</td>\n",
       "      <td>Awful</td>\n",
       "      <td>i tried this and it was just awful it tasted l...</td>\n",
       "      <td>0</td>\n",
       "      <td>[i, tried, this, and, it, was, just, awful, it...</td>\n",
       "      <td>[tried, awful, tasted, like, fizzy, cough, syr...</td>\n",
       "      <td>[tried, awful, tasted, like, fizzy, cough, syr...</td>\n",
       "      <td>tried awful tasted like fizzy cough syrup even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49516</td>\n",
       "      <td>B001E5E268</td>\n",
       "      <td>A3LOIAZYY3U9V9</td>\n",
       "      <td>Adam Z</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>1172707200</td>\n",
       "      <td>Go for Pinhead Gunpowder instead</td>\n",
       "      <td>having purchased a package of this tea and one...</td>\n",
       "      <td>0</td>\n",
       "      <td>[having, purchased, a, package, of, this, tea,...</td>\n",
       "      <td>[purchased, package, tea, one, stash, pinhead,...</td>\n",
       "      <td>[purchased, package, tea, one, stash, pinhead,...</td>\n",
       "      <td>purchased package tea one stash pinhead gunpow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>546407</td>\n",
       "      <td>B0014GHZ3O</td>\n",
       "      <td>A3UHFQT4E3R2D3</td>\n",
       "      <td>a consumer</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1305936000</td>\n",
       "      <td>Poor excuse for an Easter gift</td>\n",
       "      <td>hopefully by the time you consider this produc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hopefully, by, the, time, you, consider, this...</td>\n",
       "      <td>[hopefully, time, consider, product, vendor, c...</td>\n",
       "      <td>[hopefully, time, consider, product, vendor, c...</td>\n",
       "      <td>hopefully time consider product vendor changed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>357948</td>\n",
       "      <td>B0015J7BG6</td>\n",
       "      <td>AFX0Z8Q4XXW0F</td>\n",
       "      <td>Michael</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1297123200</td>\n",
       "      <td>Big disappointment</td>\n",
       "      <td>these marshmallows are no larger than standard...</td>\n",
       "      <td>0</td>\n",
       "      <td>[these, marshmallows, are, no, larger, than, s...</td>\n",
       "      <td>[marshmallows, larger, standard, jet, puffed, ...</td>\n",
       "      <td>[marshmallow, larger, standard, jet, puffed, c...</td>\n",
       "      <td>marshmallow larger standard jet puffed campfir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id   ProductId          UserId    ProfileName  \\\n",
       "labels                                                        \n",
       "0      0  100942  B0016ZU83M  A3E3EQ39FZTZEK         sh sh    \n",
       "       1  480042  B003IHO8OG  A11JJPVSHVJM0S  Nicole Taylor   \n",
       "       2   49516  B001E5E268  A3LOIAZYY3U9V9         Adam Z   \n",
       "       3  546407  B0014GHZ3O  A3UHFQT4E3R2D3     a consumer   \n",
       "       4  357948  B0015J7BG6   AFX0Z8Q4XXW0F        Michael   \n",
       "\n",
       "          HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "labels                                                                      \n",
       "0      0                     0                       0      2  1347667200   \n",
       "       1                     1                       7      1  1287705600   \n",
       "       2                    10                      11      2  1172707200   \n",
       "       3                     2                       2      1  1305936000   \n",
       "       4                     0                       0      1  1297123200   \n",
       "\n",
       "                                        Summary  \\\n",
       "labels                                            \n",
       "0      0  Much less flaovr nearly same calories   \n",
       "       1                                  Awful   \n",
       "       2      Go for Pinhead Gunpowder instead    \n",
       "       3         Poor excuse for an Easter gift   \n",
       "       4                     Big disappointment   \n",
       "\n",
       "                                                       Text  labels  \\\n",
       "labels                                                                \n",
       "0      0  these are for people that cannot tolerate suga...       0   \n",
       "       1  i tried this and it was just awful it tasted l...       0   \n",
       "       2  having purchased a package of this tea and one...       0   \n",
       "       3  hopefully by the time you consider this produc...       0   \n",
       "       4  these marshmallows are no larger than standard...       0   \n",
       "\n",
       "                                             tokenized_text  \\\n",
       "labels                                                        \n",
       "0      0  [these, are, for, people, that, can, not, tole...   \n",
       "       1  [i, tried, this, and, it, was, just, awful, it...   \n",
       "       2  [having, purchased, a, package, of, this, tea,...   \n",
       "       3  [hopefully, by, the, time, you, consider, this...   \n",
       "       4  [these, marshmallows, are, no, larger, than, s...   \n",
       "\n",
       "                                          stop_removed_text  \\\n",
       "labels                                                        \n",
       "0      0  [people, tolerate, sugar, low, cal, slightly, ...   \n",
       "       1  [tried, awful, tasted, like, fizzy, cough, syr...   \n",
       "       2  [purchased, package, tea, one, stash, pinhead,...   \n",
       "       3  [hopefully, time, consider, product, vendor, c...   \n",
       "       4  [marshmallows, larger, standard, jet, puffed, ...   \n",
       "\n",
       "                                            lemmatized_text  \\\n",
       "labels                                                        \n",
       "0      0  [people, tolerate, sugar, low, cal, slightly, ...   \n",
       "       1  [tried, awful, tasted, like, fizzy, cough, syr...   \n",
       "       2  [purchased, package, tea, one, stash, pinhead,...   \n",
       "       3  [hopefully, time, consider, product, vendor, c...   \n",
       "       4  [marshmallow, larger, standard, jet, puffed, c...   \n",
       "\n",
       "                                               cleaned_text  \n",
       "labels                                                       \n",
       "0      0  people tolerate sugar low cal slightly lower c...  \n",
       "       1  tried awful tasted like fizzy cough syrup even...  \n",
       "       2  purchased package tea one stash pinhead gunpow...  \n",
       "       3  hopefully time consider product vendor changed...  \n",
       "       4  marshmallow larger standard jet puffed campfir...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean text by removing punctuations and special characters and convert string to lower case\n",
    "sampled_data = sampled_data.replace(r'[^A-Za-z0-9]+', ' ', regex=True)\n",
    "sampled_data['Text'] = sampled_data['Text'].str.lower()\n",
    "\n",
    "# tokenize text\n",
    "sampled_data['tokenized_text'] = sampled_data['Text'].apply(nltk.tokenize.word_tokenize)\n",
    "\n",
    "# remove stop words\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "sampled_data['stop_removed_text'] = sampled_data['tokenized_text'].apply(lambda sentence: [word for word in sentence if word not in stop_words])\n",
    "\n",
    "# lemmatize tokens\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "sampled_data['lemmatized_text'] = sampled_data['stop_removed_text'].apply(lambda sentence: [lemmatizer.lemmatize(word) for word in sentence])\n",
    "\n",
    "# clean tokens into one string\n",
    "sampled_data['cleaned_text'] = sampled_data['lemmatized_text'].apply(lambda sentence: ' '.join([word for word in sentence]))\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf feature set\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# split data into training and testing set with 70-30 split ratio\n",
    "train_x, test_x, train_y, test_y = train_test_split(sampled_data['cleaned_text'], sampled_data['labels'], test_size=0.3)\n",
    "\n",
    "tfidf_train_x = tfidf_vect.fit_transform(train_x)\n",
    "tfidf_test_x = tfidf_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114851, 54734)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression on TF-IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8814983443708609\n",
      "Recall score:\t\t0.8672123137063279\n",
      "Accuracy score:\t\t0.8755866160128395\n",
      "F1 Score:\t\t0.8742969744242375\n"
     ]
    }
   ],
   "source": [
    "# perform logistic regresstion model on TF-IDF feature\n",
    "tfidf_log = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# train model on training set\n",
    "tfidf_log.fit(tfidf_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = tfidf_log.predict(tfidf_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier on TF-IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8814304144492824\n",
      "Recall score:\t\t0.890259793142764\n",
      "Accuracy score:\t\t0.8855006805761534\n",
      "F1 Score:\t\t0.8858231027916211\n"
     ]
    }
   ],
   "source": [
    "# perform multinomial naive bayes model on TF-IDF feature\n",
    "tfidf_rfc = RandomForestClassifier()\n",
    "\n",
    "# train model on training set\n",
    "tfidf_rfc.fit(tfidf_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = tfidf_rfc.predict(tfidf_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine on TF-IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8847169499917478\n",
      "Recall score:\t\t0.8731167033146021\n",
      "Accuracy score:\t\t0.8799341771123256\n",
      "F1 Score:\t\t0.8788785506414722\n"
     ]
    }
   ],
   "source": [
    "# perform support vector machine model on TF-IDF feature\n",
    "tfidf_svm = LinearSVC()\n",
    "\n",
    "# train model on training set\n",
    "tfidf_svm.fit(tfidf_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = tfidf_svm.predict(tfidf_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word2vec feature from google embeddings\n",
    "sampled_data['embedding_text'] = sampled_data['lemmatized_text'].apply(lambda sentence: [embeddings[word] for word in sentence if word in embeddings])\n",
    "sampled_data['word2vec_text'] = sampled_data['embedding_text'].apply(lambda arr: np.mean(arr, axis=0))\n",
    "word2vec_data = pd.DataFrame(np.vstack(sampled_data['word2vec_text'].values))\n",
    "\n",
    "# split data into training and testing set with 70-30 split ratio\n",
    "word2vec_train_x, word2vec_test_x, train_y, test_y = train_test_split(word2vec_data, sampled_data['labels'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression on Word2Vec Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8233045212765957\n",
      "Recall score:\t\t0.8071376191640186\n",
      "Accuracy score:\t\t0.8174430652337322\n",
      "F1 Score:\t\t0.8151409175066859\n"
     ]
    }
   ],
   "source": [
    "# perform logistic regresstion model on TF-IDF feature\n",
    "word2vec_log = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# train model on training set\n",
    "word2vec_log.fit(word2vec_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = word2vec_log.predict(word2vec_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier on Word2Vec Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8352299856756327\n",
      "Recall score:\t\t0.8551698851136641\n",
      "Accuracy score:\t\t0.8436503260670825\n",
      "F1 Score:\t\t0.84508233020653\n"
     ]
    }
   ],
   "source": [
    "# perform multinomial naive bayes model on TF-IDF feature\n",
    "word2vec_rfc = RandomForestClassifier()\n",
    "\n",
    "# train model on training set\n",
    "word2vec_rfc.fit(word2vec_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = word2vec_rfc.predict(word2vec_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine on Word2Vec Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8241203935300984\n",
      "Recall score:\t\t0.8053858062413428\n",
      "Accuracy score:\t\t0.8172399081730085\n",
      "F1 Score:\t\t0.8146454032224832\n"
     ]
    }
   ],
   "source": [
    "# perform support vector machine model on TF-IDF feature\n",
    "word2vec_svm = LinearSVC()\n",
    "\n",
    "# train model on training set\n",
    "word2vec_svm.fit(word2vec_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = word2vec_svm.predict(word2vec_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit ('3.9.14')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50662f09ec6209d433ff0029bf9cc367be01420c8c5568a1e0b3ae42a6d5264"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
