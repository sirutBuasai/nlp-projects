{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 525 Assignment 2\n",
    "Sirut Buasai, sbuasai2@wpi.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils import resample\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "embeddings = gensim_api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval and Processing\n",
    "#### Remove Unused Columns and Create Labels Based On Rating score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    486417\n",
       "0     82037\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from csv file\n",
    "raw_data = pd.read_csv('Reviews.csv')\n",
    "\n",
    "# drop unused columns\n",
    "raw_data.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time', 'Summary'], axis=1, inplace=True)\n",
    "\n",
    "# create labels based on score (label 1 when score >= 3, label 0 when score < 3)\n",
    "raw_data['labels'] = np.where(raw_data.Score >= 3, 1, 0)\n",
    "raw_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>labels</th>\n",
       "      <th>lmao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>0</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "      <td>great taffy at a great price.  there was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>5</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "      <td>1</td>\n",
       "      <td>great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>2</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "      <td>0</td>\n",
       "      <td>i'm disappointed with the flavor. the chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>5</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "      <td>1</td>\n",
       "      <td>these stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>5</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "      <td>1</td>\n",
       "      <td>these are the best treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>5</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "      <td>1</td>\n",
       "      <td>i am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568454 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Score                                               Text  labels  \\\n",
       "0           5  I have bought several of the Vitality canned d...       1   \n",
       "1           1  Product arrived labeled as Jumbo Salted Peanut...       0   \n",
       "2           4  This is a confection that has been around a fe...       1   \n",
       "3           2  If you are looking for the secret ingredient i...       0   \n",
       "4           5  Great taffy at a great price.  There was a wid...       1   \n",
       "...       ...                                                ...     ...   \n",
       "568449      5  Great for sesame chicken..this is a good if no...       1   \n",
       "568450      2  I'm disappointed with the flavor. The chocolat...       0   \n",
       "568451      5  These stars are small, so you can give 10-15 o...       1   \n",
       "568452      5  These are the BEST treats for training and rew...       1   \n",
       "568453      5  I am very satisfied ,product is as advertised,...       1   \n",
       "\n",
       "                                                     lmao  \n",
       "0       i have bought several of the vitality canned d...  \n",
       "1       product arrived labeled as jumbo salted peanut...  \n",
       "2       this is a confection that has been around a fe...  \n",
       "3       if you are looking for the secret ingredient i...  \n",
       "4       great taffy at a great price.  there was a wid...  \n",
       "...                                                   ...  \n",
       "568449  great for sesame chicken..this is a good if no...  \n",
       "568450  i'm disappointed with the flavor. the chocolat...  \n",
       "568451  these stars are small, so you can give 10-15 o...  \n",
       "568452  these are the best treats for training and rew...  \n",
       "568453  i am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[568454 rows x 4 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['lmao'] = raw_data['Text'].apply(lambda row: row.lower())\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Balanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    40000\n",
       "0    40000\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample balanced data\n",
    "ones = raw_data[raw_data['labels'] == 1]\n",
    "zeros = raw_data[raw_data['labels'] == 0]\n",
    "\n",
    "# balance ones and zeros\n",
    "resampled_ones = resample(ones, replace=True, n_samples=int(40000))\n",
    "resampled_zeros = resample(zeros, replace=True, n_samples=int(40000))\n",
    "\n",
    "sampled_data = pd.concat([resampled_ones, resampled_zeros])\n",
    "sampled_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the Data and Reset the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>labels</th>\n",
       "      <th>lmao</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>While the flowers were cute...they looked noth...</td>\n",
       "      <td>0</td>\n",
       "      <td>while the flowers were cute...they looked noth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>All the seeds started are growing.  The first ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all the seeds started are growing.  the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>I love the Keurig coffee maker, but it seemed ...</td>\n",
       "      <td>1</td>\n",
       "      <td>i love the keurig coffee maker, but it seemed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>A great way to buy groceries if u cannot get o...</td>\n",
       "      <td>1</td>\n",
       "      <td>a great way to buy groceries if u cannot get o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A number of us earnestly tried this product bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>a number of us earnestly tried this product bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text  labels  \\\n",
       "0      2  While the flowers were cute...they looked noth...       0   \n",
       "1      5  All the seeds started are growing.  The first ...       1   \n",
       "2      5  I love the Keurig coffee maker, but it seemed ...       1   \n",
       "3      5  A great way to buy groceries if u cannot get o...       1   \n",
       "4      1  A number of us earnestly tried this product bu...       0   \n",
       "\n",
       "                                                lmao  \n",
       "0  while the flowers were cute...they looked noth...  \n",
       "1  all the seeds started are growing.  the first ...  \n",
       "2  i love the keurig coffee maker, but it seemed ...  \n",
       "3  a great way to buy groceries if u cannot get o...  \n",
       "4  a number of us earnestly tried this product bu...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle and reset index\n",
    "sampled_data = sampled_data.sample(frac=1).reset_index(drop=True)\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuations, Tokenize, Remove Stop Words, and Lemmatize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>labels</th>\n",
       "      <th>lmao</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stop_removed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>while the flowers were cute they looked nothin...</td>\n",
       "      <td>0</td>\n",
       "      <td>while the flowers were cute they looked nothin...</td>\n",
       "      <td>[while, the, flowers, were, cute, they, looked...</td>\n",
       "      <td>[flowers, cute, looked, nothing, like, picture...</td>\n",
       "      <td>[flower, cute, looked, nothing, like, picture,...</td>\n",
       "      <td>flower cute looked nothing like picture none f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>all the seeds started are growing the first se...</td>\n",
       "      <td>1</td>\n",
       "      <td>all the seeds started are growing the first se...</td>\n",
       "      <td>[all, the, seeds, started, are, growing, the, ...</td>\n",
       "      <td>[seeds, started, growing, first, set, leaves, ...</td>\n",
       "      <td>[seed, started, growing, first, set, leaf, sta...</td>\n",
       "      <td>seed started growing first set leaf started an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>i love the keurig coffee maker but it seemed s...</td>\n",
       "      <td>1</td>\n",
       "      <td>i love the keurig coffee maker but it seemed s...</td>\n",
       "      <td>[i, love, the, keurig, coffee, maker, but, it,...</td>\n",
       "      <td>[love, keurig, coffee, maker, seemed, silly, s...</td>\n",
       "      <td>[love, keurig, coffee, maker, seemed, silly, s...</td>\n",
       "      <td>love keurig coffee maker seemed silly spend ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>a great way to buy groceries if u cannot get o...</td>\n",
       "      <td>1</td>\n",
       "      <td>a great way to buy groceries if u cannot get o...</td>\n",
       "      <td>[a, great, way, to, buy, groceries, if, u, can...</td>\n",
       "      <td>[great, way, buy, groceries, u, get, need, shi...</td>\n",
       "      <td>[great, way, buy, grocery, u, get, need, ship,...</td>\n",
       "      <td>great way buy grocery u get need ship someone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>a number of us earnestly tried this product bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>a number of us earnestly tried this product bu...</td>\n",
       "      <td>[a, number, of, us, earnestly, tried, this, pr...</td>\n",
       "      <td>[number, us, earnestly, tried, product, trying...</td>\n",
       "      <td>[number, u, earnestly, tried, product, trying,...</td>\n",
       "      <td>number u earnestly tried product trying anothe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text  labels  \\\n",
       "0      2  while the flowers were cute they looked nothin...       0   \n",
       "1      5  all the seeds started are growing the first se...       1   \n",
       "2      5  i love the keurig coffee maker but it seemed s...       1   \n",
       "3      5  a great way to buy groceries if u cannot get o...       1   \n",
       "4      1  a number of us earnestly tried this product bu...       0   \n",
       "\n",
       "                                                lmao  \\\n",
       "0  while the flowers were cute they looked nothin...   \n",
       "1  all the seeds started are growing the first se...   \n",
       "2  i love the keurig coffee maker but it seemed s...   \n",
       "3  a great way to buy groceries if u cannot get o...   \n",
       "4  a number of us earnestly tried this product bu...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [while, the, flowers, were, cute, they, looked...   \n",
       "1  [all, the, seeds, started, are, growing, the, ...   \n",
       "2  [i, love, the, keurig, coffee, maker, but, it,...   \n",
       "3  [a, great, way, to, buy, groceries, if, u, can...   \n",
       "4  [a, number, of, us, earnestly, tried, this, pr...   \n",
       "\n",
       "                                   stop_removed_text  \\\n",
       "0  [flowers, cute, looked, nothing, like, picture...   \n",
       "1  [seeds, started, growing, first, set, leaves, ...   \n",
       "2  [love, keurig, coffee, maker, seemed, silly, s...   \n",
       "3  [great, way, buy, groceries, u, get, need, shi...   \n",
       "4  [number, us, earnestly, tried, product, trying...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  [flower, cute, looked, nothing, like, picture,...   \n",
       "1  [seed, started, growing, first, set, leaf, sta...   \n",
       "2  [love, keurig, coffee, maker, seemed, silly, s...   \n",
       "3  [great, way, buy, grocery, u, get, need, ship,...   \n",
       "4  [number, u, earnestly, tried, product, trying,...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  flower cute looked nothing like picture none f...  \n",
       "1  seed started growing first set leaf started an...  \n",
       "2  love keurig coffee maker seemed silly spend ex...  \n",
       "3  great way buy grocery u get need ship someone ...  \n",
       "4  number u earnestly tried product trying anothe...  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean text by removing punctuations and special characters and convert string to lower case\n",
    "sampled_data = sampled_data.replace(r'[^A-Za-z0-9]+', ' ', regex=True)\n",
    "sampled_data['Text'] = sampled_data['Text'].str.lower()\n",
    "\n",
    "# tokenize text\n",
    "sampled_data['tokenized_text'] = sampled_data['Text'].apply(nltk.tokenize.word_tokenize)\n",
    "\n",
    "# remove stop words\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "sampled_data['stop_removed_text'] = sampled_data['tokenized_text'].apply(lambda sentence: [word for word in sentence if word not in stop_words])\n",
    "\n",
    "# lemmatize tokens\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "sampled_data['lemmatized_text'] = sampled_data['stop_removed_text'].apply(lambda sentence: [lemmatizer.lemmatize(word) for word in sentence])\n",
    "\n",
    "# clean tokens into one string\n",
    "sampled_data['cleaned_text'] = sampled_data['lemmatized_text'].apply(lambda sentence: ' '.join([word for word in sentence]))\n",
    "sampled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: TF-IDF Approach\n",
    "### TF-IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf feature set\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# split data into training and testing set with 70-30 split ratio\n",
    "train_x, test_x, train_y, test_y = train_test_split(sampled_data['cleaned_text'], sampled_data['labels'], test_size=0.3)\n",
    "\n",
    "tfidf_train_x = tfidf_vect.fit_transform(train_x)\n",
    "tfidf_test_x = tfidf_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56000, 38753)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression on TF-IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8754214430209035\n",
      "Recall score:\t\t0.8677416659704236\n",
      "Accuracy score:\t\t0.8724583333333333\n",
      "F1 Score:\t\t0.8715646372676541\n"
     ]
    }
   ],
   "source": [
    "# perform logistic regresstion model on TF-IDF feature\n",
    "tfidf_log = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# train model on training set\n",
    "tfidf_log.fit(tfidf_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = tfidf_log.predict(tfidf_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier on TF-IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8908497825359653\n",
      "Recall score:\t\t0.8898821956721531\n",
      "Accuracy score:\t\t0.8907083333333333\n",
      "F1 Score:\t\t0.8903657262277953\n"
     ]
    }
   ],
   "source": [
    "# perform multinomial naive bayes model on TF-IDF feature\n",
    "tfidf_rfc = RandomForestClassifier()\n",
    "\n",
    "# train model on training set\n",
    "tfidf_rfc.fit(tfidf_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = tfidf_rfc.predict(tfidf_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine on TF-IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8836696625402747\n",
      "Recall score:\t\t0.8707494360431114\n",
      "Accuracy score:\t\t0.878375\n",
      "F1 Score:\t\t0.8771619744981694\n"
     ]
    }
   ],
   "source": [
    "# perform support vector machine model on TF-IDF feature\n",
    "tfidf_svm = LinearSVC()\n",
    "\n",
    "# train model on training set\n",
    "tfidf_svm.fit(tfidf_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = tfidf_svm.predict(tfidf_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Word2Vec Approach\n",
    "### Word2Vec Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a word2vec feature from google embeddings\n",
    "sampled_data['embedding_text'] = sampled_data['lemmatized_text'].apply(lambda sentence: [embeddings[word] for word in sentence if word in embeddings])\n",
    "sampled_data['word2vec_text'] = sampled_data['embedding_text'].apply(lambda arr: np.mean(arr, axis=0))\n",
    "word2vec_data = pd.DataFrame(np.vstack(sampled_data['word2vec_text'].values))\n",
    "\n",
    "# split data into training and testing set with 70-30 split ratio\n",
    "word2vec_train_x, word2vec_test_x, train_y, test_y = train_test_split(word2vec_data, sampled_data['labels'], test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression on Word2Vec Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8214743315964808\n",
      "Recall score:\t\t0.8016170709344003\n",
      "Accuracy score:\t\t0.81375\n",
      "F1 Score:\t\t0.8114242321970975\n"
     ]
    }
   ],
   "source": [
    "# perform logistic regresstion model on TF-IDF feature\n",
    "word2vec_log = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# train model on training set\n",
    "word2vec_log.fit(word2vec_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = word2vec_log.predict(word2vec_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier on Word2Vec Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8507561358565408\n",
      "Recall score:\t\t0.8581311994665333\n",
      "Accuracy score:\t\t0.8538333333333333\n",
      "F1 Score:\t\t0.8544277533405261\n"
     ]
    }
   ],
   "source": [
    "# perform multinomial naive bayes model on TF-IDF feature\n",
    "word2vec_rfc = RandomForestClassifier()\n",
    "\n",
    "# train model on training set\n",
    "word2vec_rfc.fit(word2vec_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = word2vec_rfc.predict(word2vec_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine on Word2Vec Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8203465050780917\n",
      "Recall score:\t\t0.8012003000750187\n",
      "Accuracy score:\t\t0.8129166666666666\n",
      "F1 Score:\t\t0.8106603694020411\n"
     ]
    }
   ],
   "source": [
    "# perform support vector machine model on TF-IDF feature\n",
    "word2vec_svm = LinearSVC()\n",
    "\n",
    "# train model on training set\n",
    "word2vec_svm.fit(word2vec_train_x, train_y)\n",
    "\n",
    "# test model on testing set\n",
    "prediction_y = word2vec_svm.predict(word2vec_test_x)\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: BERT Without Tuning Approach\n",
    "### Initialize and Run Untuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "loading configuration file config.json from cache at /Users/sirutbuasai/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/324d3097568e82724d53d7ac1d312aa719d48037/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/sirutbuasai/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/324d3097568e82724d53d7ac1d312aa719d48037/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/sirutbuasai/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/324d3097568e82724d53d7ac1d312aa719d48037/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading configuration file config.json from cache at /Users/sirutbuasai/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/324d3097568e82724d53d7ac1d312aa719d48037/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/sirutbuasai/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/324d3097568e82724d53d7ac1d312aa719d48037/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/sirutbuasai/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/324d3097568e82724d53d7ac1d312aa719d48037/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/sirutbuasai/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/324d3097568e82724d53d7ac1d312aa719d48037/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/sirutbuasai/.cache/huggingface/hub/models--distilbert-base-uncased-finetuned-sst-2-english/snapshots/324d3097568e82724d53d7ac1d312aa719d48037/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.994321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.946179</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.996443</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.990148</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.997801</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score  predictions  labels\n",
       "0  NEGATIVE  0.994321            0       0\n",
       "1  NEGATIVE  0.946179            0       1\n",
       "2  POSITIVE  0.996443            1       1\n",
       "3  POSITIVE  0.990148            1       1\n",
       "4  NEGATIVE  0.997801            0       0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# truncate raw text since BERT only allows 512 string length\n",
    "BERT_MAX_STR = 512\n",
    "sampled_data['truncated_text'] = sampled_data['Text'].str.slice(0, BERT_MAX_STR)\n",
    "\n",
    "# initialize pretrained BERT sentiment classifier\n",
    "untuned_bert = pipeline('sentiment-analysis')\n",
    "\n",
    "# predict using BERT model\n",
    "result = untuned_bert(sampled_data['truncated_text'].tolist())\n",
    "bert_predictions = pd.DataFrame(result)\n",
    "\n",
    "# convert POSITIVE and NEGATIVE result into 1s and 0s\n",
    "bert_predictions['predictions'] = bert_predictions['label'].map({'POSITIVE' : 1, 'NEGATIVE' : 0})\n",
    "bert_predictions['labels'] = sampled_data['labels']\n",
    "bert_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Untuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision score:\t0.8902278290340803\n",
      "Recall score:\t\t0.7092\n",
      "Accuracy score:\t\t0.810875\n",
      "F1 Score:\t\t0.7894692900676259\n"
     ]
    }
   ],
   "source": [
    "# retrive test and prediction labels\n",
    "prediction_y = bert_predictions['predictions']\n",
    "test_y = bert_predictions['labels']\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: BERT With Tuning Approach\n",
    "### Initialize and Tune Tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialized based uncased BERT tokenizer and classification model\n",
    "bert_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(bert_name, do_lower_case=True)\n",
    "classifier = BertForSequenceClassification.from_pretrained(bert_name, num_labels=2)\n",
    "\n",
    "# split data into training and testing set with 70-30 split ratio\n",
    "train_x, validate_x, train_y, validate_y = train_test_split(sampled_data['Text'], sampled_data['labels'], test_size=0.3)\n",
    "train_x = train_x.reset_index(drop=True)\n",
    "validate_x = validate_x.reset_index(drop=True)\n",
    "train_y = train_y.reset_index(drop=True)\n",
    "validate_y = validate_y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data into Compatible Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the dataset\n",
    "train_tokens = tokenizer(train_x.tolist(), truncation=True, padding=True, max_length=BERT_MAX_STR)\n",
    "validate_tokens = tokenizer(validate_x.tolist(), truncation=True, padding=True, max_length=BERT_MAX_STR)\n",
    "\n",
    "# class for token arr to tensors conversion\n",
    "class NewsGroupsDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings, labels):\n",
    "    self.encodings = encodings\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "# convert tokenized list into torch tensors\n",
    "train_tensors = NewsGroupsDataset(train_tokens, train_y)\n",
    "validate_tensors = NewsGroupsDataset(validate_tokens, validate_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Training and Trainer for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics computation function\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return {\n",
    "    'accuracy': acc,\n",
    "  }\n",
    "\n",
    "# initialize training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./results',          # output directory\n",
    "  num_train_epochs=1,              # total number of training epochs\n",
    "  per_device_train_batch_size=8,  # batch size per device during training\n",
    "  per_device_eval_batch_size=10,   # batch size for evaluation\n",
    "  warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "  weight_decay=0.01,               # strength of weight decay\n",
    "  logging_dir='./logs',            # directory for storing logs\n",
    "  load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
    "  # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
    "  logging_steps=400,               # log & save weights each logging_steps\n",
    "  save_steps=400,\n",
    "  evaluation_strategy=\"steps\",     # evaluate each `logging_steps`\n",
    ")\n",
    "\n",
    "# initialize trainer\n",
    "trainer = Trainer(\n",
    "  model=classifier,                    # the instantiated Transformers model to be trained\n",
    "  args=training_args,                  # training arguments, defined above\n",
    "  train_dataset=train_tensors,         # training dataset\n",
    "  eval_dataset=validate_tensors,       # evaluation dataset\n",
    "  compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train BERT\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Test Data to Evaluate Tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance ones and zeros\n",
    "resampled_ones = resample(ones, replace=True, n_samples=int(20000))\n",
    "resampled_zeros = resample(zeros, replace=True, n_samples=int(20000))\n",
    "\n",
    "test_data = pd.concat([resampled_ones, resampled_zeros])\n",
    "test_data['labels'].value_counts()\n",
    "\n",
    "# shuffle and reset index\n",
    "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to get BERT predictions\n",
    "def get_prediction(text):\n",
    "  # prepare our text into tokenized sequence\n",
    "  inputs = tokenizer(text, padding=True, truncation=True, max_length=BERT_MAX_STR, return_tensors=\"pt\").to(\"cuda\")\n",
    "  # perform inference to our model\n",
    "  outputs = classifier(**inputs)\n",
    "  # get output probabilities by doing softmax\n",
    "  probs = outputs[0].softmax(1)\n",
    "  # executing argmax function to get the candidate label\n",
    "  return int(probs.argmax())\n",
    "\n",
    "# evaluate testing set on BERT\n",
    "test_data['predictions'] = test_data['Text'].apply(lambda text: get_prediction(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Tuned BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive test and prediction labels\n",
    "prediction_y = test_data['predictions']\n",
    "test_y = test_data['labels']\n",
    "\n",
    "print(f\"Precision score:\\t{precision_score(test_y, prediction_y)}\")\n",
    "print(f\"Recall score:\\t\\t{recall_score(test_y, prediction_y)}\")\n",
    "print(f\"Accuracy score:\\t\\t{accuracy_score(test_y, prediction_y)}\")\n",
    "print(f\"F1 Score:\\t\\t{f1_score(test_y, prediction_y)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit ('3.9.14')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50662f09ec6209d433ff0029bf9cc367be01420c8c5568a1e0b3ae42a6d5264"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
