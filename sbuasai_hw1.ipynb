{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 525 Assignment 1\n",
    "Sirut Buasai, sbuasai2@wpi.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/sirutbuasai/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLTK downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data retrieval and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv files\n",
    "real_data = pd.read_csv('True.csv')\n",
    "fake_data = pd.read_csv('Fake.csv')\n",
    "\n",
    "# clean the data by removing punctuations and special characters and convert string to lower case\n",
    "real_data = real_data.replace(r'[^A-Za-z0-9]+', ' ', regex=True)\n",
    "real_data['text'] = real_data['text'].str.lower()\n",
    "\n",
    "fake_data = fake_data.replace(r'[^A-Za-z0-9]+', ' ', regex=True)\n",
    "fake_data['text'] = fake_data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Explore Essential Information from Text Data and Preprocessing\n",
    "### Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize texts\n",
    "real_data['tokenized_text'] = real_data['text'].apply(nltk.tokenize.word_tokenize)\n",
    "real_tokens = real_data['tokenized_text'].explode()\n",
    "real_tokens.dropna(inplace=True)\n",
    "real_tokens = real_tokens.to_list()\n",
    "\n",
    "fake_data['tokenized_text'] = fake_data['text'].apply(nltk.tokenize.word_tokenize)\n",
    "fake_tokens = fake_data['tokenized_text'].explode()\n",
    "fake_tokens.dropna(inplace=True)\n",
    "fake_tokens = fake_tokens.to_list()\n",
    "\n",
    "collection_tokens = real_tokens + fake_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "real_stop = [x for x in real_tokens if x not in stop_words]\n",
    "fake_stop = [x for x in fake_tokens if x not in stop_words]\n",
    "collection_stop = [x for x in collection_tokens if x not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Lemminization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize tokens\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "real_lemm = [lemmatizer.lemmatize(x) for x in real_stop]\n",
    "fake_lemm = [lemmatizer.lemmatize(x) for x in fake_stop]\n",
    "collection_lemm = [lemmatizer.lemmatize(x) for x in collection_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Top 100 Common Words and WordCloud Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve top 100 most common words\n",
    "real_freq = nltk.FreqDist(real_lemm).most_common(100)\n",
    "fake_freq = nltk.FreqDist(fake_lemm).most_common(100)\n",
    "collection_freq = nltk.FreqDist(collection_lemm).most_common(100)\n",
    "\n",
    "# download tables to excel spreadsheet\n",
    "real_df = pd.DataFrame(data=dict(real_freq), index=[0])\n",
    "real_df = (real_df.T)\n",
    "real_df.to_excel('real_freq.xlsx')\n",
    "\n",
    "fake_df = pd.DataFrame(data=dict(fake_freq), index=[0])\n",
    "fake_df = (fake_df.T)\n",
    "fake_df.to_excel('fake_freq.xlsx')\n",
    "\n",
    "collection_df = pd.DataFrame(data=dict(collection_freq), index=[0])\n",
    "collection_df = (collection_df.T)\n",
    "collection_df.to_excel('collection_freq.xlsx')\n",
    "\n",
    "# create wordcloud for analysis\n",
    "real_wordcloud = WordCloud().generate_from_frequencies(dict(real_freq))\n",
    "plt.imshow(real_wordcloud)\n",
    "plt.show()\n",
    "\n",
    "fake_wordcloud = WordCloud().generate_from_frequencies(dict(fake_freq))\n",
    "plt.imshow(fake_wordcloud)\n",
    "plt.show()\n",
    "\n",
    "collection_wordcloud = WordCloud().generate_from_frequencies(dict(collection_freq))\n",
    "plt.imshow(collection_wordcloud)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Build Machine Learning Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('3.9.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34594e8d4529e551740a7cc1435132dbc406410467512f56bb1e1e7e935fe030"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
