{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 525 Assignment 3\n",
    "Sirut Buasai, sbuasai2@wpi.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizerFast\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval and Processing\n",
    "### Process JSON Data Format into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare load data function for preprocessing\n",
    "def load_data(path):  \n",
    "  # load the json file\n",
    "  with open(path, 'r') as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "  # initialize return lists\n",
    "  ids = []\n",
    "  contexts = []\n",
    "  questions = []\n",
    "  answers = []\n",
    "\n",
    "  # initialize empty answer dict\n",
    "  empty_ans = {'text': '',\n",
    "               'answer_start': 0}\n",
    "\n",
    "  # iterate through the json file and place each data into their respective lists\n",
    "  for data in raw_data['data']:\n",
    "    for topic in data['paragraphs']:\n",
    "      context = topic['context']\n",
    "      for qa in topic['qas']:\n",
    "        question = qa['question']\n",
    "        qid = qa['id']\n",
    "\n",
    "        # # if there is no answer, append empty string\n",
    "        # if not qa['answers']:\n",
    "        #   contexts.append(context)\n",
    "        #   questions.append(question)\n",
    "        #   answers.append(empty_ans)\n",
    "        #   ids.append(qid)\n",
    "          \n",
    "        # else:\n",
    "        for answer in qa['answers']:\n",
    "          contexts.append(context)\n",
    "          questions.append(question)\n",
    "          answers.append(answer)\n",
    "          ids.append(qid)\n",
    "\n",
    "  # initialize dataframe\n",
    "  df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'context': contexts,\n",
    "    'question': questions,\n",
    "    'answer': answers\n",
    "  })\n",
    "\n",
    "  return df\n",
    "\n",
    "# initialize dataset files\n",
    "train_json = 'train-v2.0.json'\n",
    "test_json = 'dev-v2.0.json'\n",
    "\n",
    "# load data\n",
    "train_data = load_data(train_json)\n",
    "test_data = load_data(test_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Subset of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a subset of train data for faster training\n",
    "train_size = int(0.1*len(train_data))\n",
    "train_data = train_data[:train_size]\n",
    "\n",
    "# sample a subset of test data (keep full size for final predictions)\n",
    "test_size = int(len(test_data))\n",
    "test_data = test_data[:test_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create End Index for Each Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare add end index function for each answer\n",
    "def add_end_idx(answers, contexts):\n",
    "  # get starting and ending index\n",
    "  for answer, context in zip(answers, contexts):\n",
    "    answer_text = answer['text']\n",
    "    start_idx = answer['answer_start']\n",
    "    end_idx = start_idx + len(answer_text)\n",
    "\n",
    "    # auto adjust in case answers length are offset\n",
    "    if context[start_idx:end_idx] == answer_text:\n",
    "      answer['answer_end'] = end_idx\n",
    "\n",
    "    # answers are off by 1\n",
    "    elif context[start_idx-1:end_idx-1] == answer_text:\n",
    "      answer['answer_start'] = start_idx - 1\n",
    "      answer['answer_end'] = end_idx - 1\n",
    "\n",
    "    # answers are off by 2\n",
    "    elif context[start_idx-2:end_idx-2] == answer_text:\n",
    "      answer['answer_start'] = start_idx - 2\n",
    "      answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "# add end index for train and test dataset\n",
    "add_end_idx(train_data['answer'], train_data['context'])\n",
    "add_end_idx(test_data['answer'], test_data['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Dataset Based on Context and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# initialize dataset encodings\n",
    "train_encodings = tokenizer(list(train_data['context']), list(train_data['question']), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(test_data['context']), list(test_data['question']), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Each Answer Starting and Ending Index Positions as Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [111], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m   encodings\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mstarts\u001b[39m\u001b[39m'\u001b[39m: starts, \u001b[39m'\u001b[39m\u001b[39mends\u001b[39m\u001b[39m'\u001b[39m: ends})\n\u001b[1;32m     20\u001b[0m \u001b[39m# add positional tokens to training and testing set\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m add_token_positions(train_encodings, train_data[\u001b[39m'\u001b[39;49m\u001b[39manswer\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     22\u001b[0m add_token_positions(test_encodings, test_data[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn [111], line 10\u001b[0m, in \u001b[0;36madd_token_positions\u001b[0;34m(encodings, answers)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(answers)):\n\u001b[1;32m      9\u001b[0m   starts\u001b[39m.\u001b[39mappend(encodings\u001b[39m.\u001b[39mchar_to_token(i, answers[i][\u001b[39m'\u001b[39m\u001b[39manswer_start\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[0;32m---> 10\u001b[0m   ends\u001b[39m.\u001b[39mappend(encodings\u001b[39m.\u001b[39;49mchar_to_token(i, answers[i][\u001b[39m'\u001b[39;49m\u001b[39manswer_end\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m))\n\u001b[1;32m     12\u001b[0m   \u001b[39m# handle truncated answers\u001b[39;00m\n\u001b[1;32m     13\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m starts[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.14/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:570\u001b[0m, in \u001b[0;36mBatchEncoding.char_to_token\u001b[0;34m(self, batch_or_char_index, char_index, sequence_index)\u001b[0m\n\u001b[1;32m    568\u001b[0m     batch_index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    569\u001b[0m     char_index \u001b[39m=\u001b[39m batch_or_char_index\n\u001b[0;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encodings[batch_index]\u001b[39m.\u001b[39;49mchar_to_token(char_index, sequence_index)\n",
      "\u001b[0;31mOverflowError\u001b[0m: can't convert negative int to unsigned"
     ]
    }
   ],
   "source": [
    "# declare add answer index positions to token encodings\n",
    "def add_token_positions(encodings, answers):\n",
    "  # initialize starting and ending encoding positions\n",
    "  starts = []\n",
    "  ends = []\n",
    "\n",
    "  # populate encoding positions\n",
    "  for i in range(len(answers)):\n",
    "    starts.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "    ends.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "\n",
    "    # handle truncated answers\n",
    "    if not starts[-1]:\n",
    "      starts[-1] = tokenizer.model_max_length\n",
    "    if not ends[-1]:\n",
    "      ends[-1] = tokenizer.model_max_length\n",
    "\n",
    "  encodings.update({'starts': starts, 'ends': ends})\n",
    "\n",
    "# add positional tokens to training and testing set\n",
    "add_token_positions(train_encodings, train_data['answer'])\n",
    "add_token_positions(test_encodings, test_data['answer'])\n",
    "\n",
    "# fix None type starts\n",
    "# for i in range(len(train_encodings['starts'])):\n",
    "#   if not train_encodings['starts'][i]:\n",
    "#     train_encodings['starts'][i] = train_encodings['ends'][i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "None in train_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataloaders for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create custom Dataset class for torch Dataloader\n",
    "class QnA_Dataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, encodings):\n",
    "    self.encodings = encodings\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.encodings.input_ids)\n",
    "\n",
    "# convert dataset to torch Dataset\n",
    "train_dataset = QnA_Dataset(train_encodings)\n",
    "test_dataset = QnA_Dataset(test_encodings)\n",
    "\n",
    "# create the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Question and Answering BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1: 100%|██████████| 1/1 [00:01<00:00,  1.21s/it, loss=5.38]\n",
      "Epoch 2: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it, loss=4.68]\n",
      "Epoch 3: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it, loss=4.14]\n",
      "Epoch 4: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it, loss=3.51]\n",
      "Epoch 5: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it, loss=2.93]\n"
     ]
    }
   ],
   "source": [
    "# create device to force GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# initalize model and optimizer\n",
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# training loop \n",
    "epoch = 10\n",
    "for epoch in range(epoch):\n",
    "  loop = tqdm(train_loader, leave=True)\n",
    "  for batch in loop:\n",
    "    optim.zero_grad()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    starts = batch['starts'].to(device)\n",
    "    ends = batch['ends'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=starts, end_positions=ends)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    loop.set_description(f'Epoch {epoch+1}')\n",
    "    loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Question and Answering BERT\n",
    "### Declare Evaluation Functions for SQuAD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(context, question):\n",
    "  inputs = tokenizer.encode_plus(question, context, return_tensors='pt', truncation=True, max_length=512).to(device)\n",
    "  outputs = model(**inputs)\n",
    "  \n",
    "  answer_start = torch.argmax(outputs[0])  \n",
    "  answer_end = torch.argmax(outputs[1]) + 1 \n",
    "  \n",
    "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "  \n",
    "  return answer\n",
    "\n",
    "def normalize_text(s):\n",
    "  \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "  import string, re\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "    return re.sub(regex, \" \", text)\n",
    "  def white_space_fix(text):\n",
    "    return \" \".join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return \"\".join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "  pred_tokens = normalize_text(prediction).split()\n",
    "  truth_tokens = normalize_text(truth).split()\n",
    "  \n",
    "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "    return int(pred_tokens == truth_tokens)\n",
    "  \n",
    "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "  \n",
    "  # if there are no common tokens then f1 = 0\n",
    "  if len(common_tokens) == 0:\n",
    "    return 0\n",
    "  \n",
    "  prec = len(common_tokens) / len(pred_tokens)\n",
    "  rec = len(common_tokens) / len(truth_tokens)\n",
    "  \n",
    "  return round(2 * (prec * rec) / (prec + rec), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Predictions into JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "test_data['predictions'] = test_data.apply(lambda row: get_prediction(row['context'], row['question']), axis=1)\n",
    "\n",
    "# convert dataframe into json\n",
    "predictions_json = dict(zip(test_data['id'], test_data['predictions']))\n",
    "with open('pred.json', 'w') as f:\n",
    "  json.dump(predictions_json, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit ('3.9.14')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d50662f09ec6209d433ff0029bf9cc367be01420c8c5568a1e0b3ae42a6d5264"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
